{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjwGkisPRIe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Move to drive\n",
        "%cd 'drive/My Drive/Thesis/Data Experimentation'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TuDZG9kRRZV",
        "colab_type": "code",
        "outputId": "62d11169-aeb8-4f85-d9aa-d42a08c8508a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# Do imports\n",
        "! pip install keras-metrics\n",
        "from keras_metrics import categorical_precision, categorical_recall, categorical_f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "from keras import regularizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from matplotlib.pyplot import hist\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-metrics\n",
            "  Downloading https://files.pythonhosted.org/packages/32/c9/a87420da8e73de944e63a8e9cdcfb1f03ca31a7c4cdcdbd45d2cdf13275a/keras_metrics-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras-metrics) (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.16.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.3.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (3.13)\n",
            "Installing collected packages: keras-metrics\n",
            "Successfully installed keras-metrics-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqWJwYpPvdYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get relevant data for this task\n",
        "def get_data(y_labels, plot_hist=True, raw=False, categorical=True):\n",
        "  df = pd.read_csv('./data/income_data.csv', index_col=0)\n",
        "  df = df.dropna()  # Drop rows with missing data\n",
        "  X = df['str']\n",
        "  Y = df[y_labels]\n",
        "  \n",
        "  if categorical:\n",
        "    Y_list = []\n",
        "    for i, labels in Y.iterrows():\n",
        "      y_l = list(labels)\n",
        "      if raw:\n",
        "        Y_list.append(y_l)\n",
        "      else:\n",
        "        Y_list.append(y_l.index(max(y_l)))\n",
        "  else:\n",
        "    Y_list = Y\n",
        "\n",
        "  # Get histogram of Y_list\n",
        "  if plot_hist:\n",
        "    hist(Y_list)\n",
        "\n",
        "  return X, Y_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptpfwVSHhFKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess strings\n",
        "np.random.seed(31415)\n",
        "\n",
        "# Example is for politics\n",
        "\n",
        "# Read data\n",
        "X, Y = get_data(['no_political', 'conservative', 'independent', 'liberal'],\n",
        "                    categorical=False, plot_hist=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxvTAKKrvlQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform to one-hot representation\n",
        "Y_l = []\n",
        "for _, d in Y.iterrows():\n",
        "  d = list(d)\n",
        "  pol = sum(d[1:-1])\n",
        "  if pol >= d[0]:\n",
        "    Y_l.append([0, 1])\n",
        "  else:\n",
        "    Y_l.append([1, 0])\n",
        "Y_l = np.asarray(Y_l)\n",
        "print(sum(Y_l))  # Gets the number in each class\n",
        "\n",
        "# Make X TF-IDF\n",
        "vec = TfidfVectorizer(stop_words='english', binary=True)\n",
        "vec.fit(X)\n",
        "X_tf = vec.transform(X).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYuwfveIDNLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get train/test split, train on 90%\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_tf, Y_l, test_size=0.1, \n",
        "                                                    shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtkN_6wpWVms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build simple feed-forward neural network\n",
        "size = len(x_train[0])\n",
        "\n",
        "DR_LIST = [0.0, 0.2, 0.3, 0.4, 0.5]\n",
        "B_S_LIST = [32, 64, 128]\n",
        "UNITS = [64, 128, 256, 512]\n",
        "weights = {0:1, 1:1}\n",
        "classes = [0, 1]\n",
        "\n",
        "def get_hp(DR_LIST, B_S_LIST, UNITS, e=EPOCHS):\n",
        "  for d in DR_LIST:\n",
        "    for b in B_S_LIST:\n",
        "      for u in UNITS:\n",
        "        for deep in [True, False]:\n",
        "          model = Sequential()\n",
        "          model.add(Dense(input_dim=size, units=u, activation='relu', \n",
        "                          # kernel_regularizer=regularizers.l1(l)\n",
        "                          ))\n",
        "          model.add(Dropout(rate=d))\n",
        "          if deep:\n",
        "            model.add(Dense(units=u, activation='relu',\n",
        "                            # kernel_regularizer=regularizers.l1(l)\n",
        "                            ))\n",
        "            model.add(Dropout(rate=d))\n",
        "          model.add(Dense(units=len(classes), activation='softmax',\n",
        "                          # kernel_regularizer=regularizers.l1(l)\n",
        "                          ))\n",
        "          model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', categorical_f1_score()])\n",
        "\n",
        "          res = model.fit(x_train, y_train, batch_size=b, epochs=e, \n",
        "                          validation_split=0.1,\n",
        "                          class_weight=weights, verbose=0\n",
        "                          )\n",
        "          print(\"Deep: %d, Dropout: %f, Batch Size: %d, Units: %d, Val Accuracy: %f Val F1 %f\" % \n",
        "                (deep, d, b, u, res.history['val_acc'][-1], res.history['val_f1_score'][-1]))\n",
        "      \n",
        "\n",
        "def train_and_save_final(d, b, u, filename, deep=True, save=True):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(input_dim=size, units=u, activation='relu'))\n",
        "        model.add(Dropout(rate=d))\n",
        "        if deep:\n",
        "          model.add(Dense(units=u, activation='relu',\n",
        "                          ))\n",
        "          model.add(Dropout(rate=d))\n",
        "        model.add(Dense(units=len(classes), activation='softmax'))\n",
        "        # Cannot save model with the keras-metrics metrics active\n",
        "        if not save:\n",
        "          model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'\n",
        "                                                                                    ,\n",
        "                                                                                    categorical_precision(), \n",
        "                                                                                    categorical_recall(), \n",
        "                                                                                    categorical_f1_score()\n",
        "                                                                                   ])\n",
        "        else:\n",
        "          model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "        res = model.fit(x_train, y_train, batch_size=b, epochs=EPOCHS, \n",
        "                        class_weight=weights, verbose=1\n",
        "                        )\n",
        "        score = model.evaluate(x_test, y_test, batch_size=b)\n",
        "        print(score)\n",
        "        if save:\n",
        "          model.save(filename)\n",
        "        return model\n",
        "\n",
        "def validate(d, b, u, e=EPOCHS, deep=False, l=0.0001):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(input_dim=size, units=u, activation='relu', \n",
        "                        # kernel_regularizer=regularizers.l1(l)\n",
        "                        ))\n",
        "        model.add(Dropout(rate=d))\n",
        "        if deep:\n",
        "          model.add(Dense(units=u, activation='relu',\n",
        "                          # kernel_regularizer=regularizers.l1(l)\n",
        "                          ))\n",
        "          model.add(Dropout(rate=d))\n",
        "        model.add(Dense(units=len(classes), activation='softmax',\n",
        "                        # kernel_regularizer=regularizers.l1(l)\n",
        "                        ))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', \n",
        "                                                                                  categorical_precision(), \n",
        "                                                                                  categorical_recall(), \n",
        "                                                                                  categorical_f1_score()\n",
        "                                                                                  ])\n",
        "\n",
        "        res = model.fit(x_train, y_train, batch_size=b, epochs=e, \n",
        "                        validation_split=0.1,\n",
        "                        class_weight=weights, verbose=1\n",
        "                        )\n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Oyeu8ItCAfC",
        "colab_type": "code",
        "outputId": "09480067-0b15-4f43-8a88-d1ab6177fa10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "get_hp(DR_LIST, B_S_LIST, UNITS)\n",
        "model = train_and_save_final(d=0.3, b=32, u=128, deep=True, filename='./models/pol.h5', save=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "4670/4670 [==============================] - 6s 1ms/step - loss: 0.6904 - acc: 0.5248\n",
            "Epoch 2/20\n",
            "4670/4670 [==============================] - 3s 716us/step - loss: 0.6148 - acc: 0.6803\n",
            "Epoch 3/20\n",
            "4670/4670 [==============================] - 3s 711us/step - loss: 0.2143 - acc: 0.9278\n",
            "Epoch 4/20\n",
            "4670/4670 [==============================] - 3s 712us/step - loss: 0.0564 - acc: 0.9827\n",
            "Epoch 5/20\n",
            "4670/4670 [==============================] - 3s 719us/step - loss: 0.0133 - acc: 0.9974\n",
            "Epoch 6/20\n",
            "4670/4670 [==============================] - 3s 729us/step - loss: 0.0085 - acc: 0.9981\n",
            "Epoch 7/20\n",
            "4670/4670 [==============================] - 3s 728us/step - loss: 0.0073 - acc: 0.9987\n",
            "Epoch 8/20\n",
            "4670/4670 [==============================] - 3s 728us/step - loss: 0.0029 - acc: 0.9996\n",
            "Epoch 9/20\n",
            "4670/4670 [==============================] - 3s 724us/step - loss: 0.0059 - acc: 0.9985\n",
            "Epoch 10/20\n",
            "4670/4670 [==============================] - 3s 718us/step - loss: 0.0040 - acc: 0.9994\n",
            "Epoch 11/20\n",
            "4670/4670 [==============================] - 3s 733us/step - loss: 0.0019 - acc: 0.9996\n",
            "Epoch 12/20\n",
            "4670/4670 [==============================] - 3s 728us/step - loss: 0.0023 - acc: 0.9994\n",
            "Epoch 13/20\n",
            "4670/4670 [==============================] - 3s 715us/step - loss: 0.0035 - acc: 0.9989\n",
            "Epoch 14/20\n",
            "4670/4670 [==============================] - 3s 724us/step - loss: 0.0024 - acc: 0.9989\n",
            "Epoch 15/20\n",
            "4670/4670 [==============================] - 3s 718us/step - loss: 0.0065 - acc: 0.9976\n",
            "Epoch 16/20\n",
            "4670/4670 [==============================] - 3s 714us/step - loss: 0.0035 - acc: 0.9989\n",
            "Epoch 17/20\n",
            "4670/4670 [==============================] - 3s 727us/step - loss: 0.0035 - acc: 0.9989\n",
            "Epoch 18/20\n",
            "4670/4670 [==============================] - 3s 721us/step - loss: 0.0019 - acc: 0.9994\n",
            "Epoch 19/20\n",
            "4670/4670 [==============================] - 3s 731us/step - loss: 0.0018 - acc: 0.9994\n",
            "Epoch 20/20\n",
            "4670/4670 [==============================] - 3s 714us/step - loss: 0.0053 - acc: 0.9983\n",
            "519/519 [==============================] - 1s 2ms/step\n",
            "[2.79722146537722, 0.5260115607510643]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAAFXYLcCesl",
        "colab_type": "code",
        "outputId": "0097eeb9-c8ee-4b82-fb11-d6ce4caa0d2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('./models/pol.h5')  # Make sure we can load the model\n",
        "print(model.evaluate(x_test, y_test))\n",
        "\n",
        "# Ensure that it is not always predicting one class\n",
        "pred = model.predict(x_test)\n",
        "p = []\n",
        "for labels in pred:\n",
        "  y_l = list(labels)\n",
        "  p.append(y_l.index(max(y_l)))\n",
        "\n",
        "r = [0, 0]\n",
        "for t in p:\n",
        "  r[t] += 1\n",
        "\n",
        "print(r)  # Number of predictions for each class\n",
        "print(sum(y_test))  # Number of each class in test set "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "519/519 [==============================] - 0s 348us/step\n",
            "[2.79722146537722, 0.5260115607510643]\n",
            "[155, 364]\n",
            "[247 272]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21e2hShcPdX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An abandoned attempt to use focal loss to compensate for class imbalance\n",
        "\n",
        "# # Implementation of focal loss - implemented in https://github.com/umbertogriffo/focal-loss-keras/blob/master/losses.py\n",
        "# def focal_loss(gamma=2, alpha=.25):\n",
        "#   def focal_loss_fixed(y_true, y_pred):\n",
        "\n",
        "#     # Scale predictions so that the class probas of each sample sum to 1\n",
        "#     y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "\n",
        "#     # Clip the prediction value to prevent NaN's and Inf's\n",
        "#     epsilon = K.epsilon()\n",
        "#     y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
        "\n",
        "#     # Calculate Cross Entropy\n",
        "#     cross_entropy = -y_true * K.log(y_pred)\n",
        "\n",
        "#     # Calculate Focal Loss\n",
        "#     loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
        "\n",
        "#     # Sum the losses in mini_batch\n",
        "#     return K.sum(loss, axis=1)\n",
        "\n",
        "#   return focal_loss_fixed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOKwYi4Mqq7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Functions used to generate ./data/income_data.csv\n",
        "# # Read dictionary from file `filename` and return `index_to_word` and\n",
        "# # `word_to_index` dictionaries.\n",
        "# # Default `filename` is for the income dictionary.\n",
        "# def get_dict(filename='./income_dataset/dictionary.txt'):\n",
        "#   index_to_word = {}\n",
        "#   word_to_index = {}\n",
        "  \n",
        "#   with open(filename, 'r') as file:\n",
        "#     for row in file:\n",
        "#       index, word = tuple(row.split(' '))\n",
        "#       index = int(index)\n",
        "#       word = word.strip()\n",
        "#       index_to_word[index] = word\n",
        "#       word_to_index[word] = index\n",
        "      \n",
        "#   # Add UNK for unknown values\n",
        "#   index_to_word[0] = 'UNK'\n",
        "#   word_to_index['UNK'] = 0\n",
        "      \n",
        "#   return index_to_word, word_to_index\n",
        "\n",
        "# # Convert unigram frequency count for one user provided by `data` to a vector.\n",
        "# # Reserves location 0 for out of vocabulary words.\n",
        "# # If `output_type` is `raw`, the raw unigram frequencies are returned.\n",
        "# # If `output_type` is `binary`, 1 is returned if the unigram appears and 0 otherwise.\n",
        "# # `vec_size` is the size of the dictionary + 1. By default it is 71556, the size of\n",
        "# # `dictionary.txt` for the income data.\n",
        "# def get_vec(data, output_type='raw', vec_size=71556):\n",
        "#   # Check that output_type is valid\n",
        "#   if output_type not in ['raw', 'binary']:\n",
        "#     raise Exception('Invalid vector type supplied. Choose raw or binary.')\n",
        "      \n",
        "#   # Get scores\n",
        "#   scores = np.zeros(vec_size, dtype=int)\n",
        "#   data_list = data.split(' ')\n",
        "  \n",
        "#   for row in data_list:\n",
        "#     index, count = row.split(':')\n",
        "#     if output_type == 'raw':\n",
        "#       scores[int(index)] = int(count)\n",
        "#     elif output_type == 'binary':\n",
        "#       scores[int(index)] = 1\n",
        "   \n",
        "#   return scores\n",
        "\n",
        "# # Read from file `filename` the user_id and unigram data. Return a list of\n",
        "# # tuples, where each tuple has the user_id and a unigram vector.\n",
        "# # Default `filename` is the file location for the income data.\n",
        "# def get_id_uni(filename='./income_dataset/jobs-unigrams.txt', output_type='raw', \n",
        "#                vec_size=71556):\n",
        "  \n",
        "#   id_list = []\n",
        "#   with open(filename, 'r') as file:\n",
        "#     for row in file:\n",
        "#       # Get user_id\n",
        "#       user_id = re.findall('^\\d+ ', row)\n",
        "#       if len(user_id) == 1:      \n",
        "#         # Get vector\n",
        "#         user_id = int(user_id[0].strip())\n",
        "#         data = re.sub('^\\d+ ', '', row)\n",
        "#         vec = get_vec(data, output_type=output_type, vec_size=vec_size)\n",
        "#         id_list.append((user_id, vec))\n",
        "      \n",
        "#       # Two appear to be missing this data\n",
        "#       elif len(user_id) == 0:\n",
        "#         continue\n",
        "#       # If we can't parse properly, throw an error\n",
        "#       else:\n",
        "#         raise Exception(\"Unable to read user_id\")\n",
        "  \n",
        "#   return id_list\n",
        "\n",
        "\n",
        "# # Associate each unigram in the list with the label specified by `label`. Return\n",
        "# # a list with all of the unigrams and a list with all of the labels.\n",
        "# # Automatically reads data from ./income_labels.csv.\n",
        "# def match_label(data, label, labels_file='./income_labels.csv'):\n",
        "#   labels = pd.read_csv('./income_labels.csv', index_col=0)\n",
        "#   X = []\n",
        "#   Y = []\n",
        "  \n",
        "#   for row in data:\n",
        "#     u_id, uni = row\n",
        "#     X.append(uni)\n",
        "#     Y.append(labels[label].loc[u_id])\n",
        "    \n",
        "#   return X, Y\n",
        "\n",
        "\n",
        "# # Convert from counts to a string\n",
        "# def to_string(x, index_to_word):\n",
        "#   sen = \"\"\n",
        "#   for ind, val in enumerate(x.tolist()):\n",
        "#     if val > 0:\n",
        "#       word = i_2_w[ind]\n",
        "#       sen += ' '.join([word]*val) + ' '\n",
        "#   return sen.strip()\n",
        "\n",
        "# # Add strings representations to ./income_labels.csv\n",
        "# def add_strings(raw, binary, i_2_w):\n",
        "#   l = pd.read_csv('./income_labels.csv')\n",
        "#   l.rename({'Unnamed: 0': \"u_id\"})\n",
        "#   raw_counts = [None]*len(l)\n",
        "#   string = [None]*len(l)\n",
        "#   binary_counts =[None]*len(l)\n",
        "#   for row in raw:\n",
        "#     u_id, uni = row\n",
        "#     ind = l.index[l['u_id'] == u_id][0]\n",
        "#     raw_counts[ind] = uni\n",
        "#     string[ind] = to_string(uni, i_2_w)\n",
        "#   for row in binary:\n",
        "#     u_id, uni = row\n",
        "#     ind = l.index[l['u_id'] == u_id][0]\n",
        "#     binary_counts[ind] = uni\n",
        "\n",
        "#   l['raw'] = raw_counts\n",
        "#   l['str'] = string\n",
        "#   l['bin'] = binary_counts\n",
        "\n",
        "  \n",
        "#   l.to_csv('./income_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}